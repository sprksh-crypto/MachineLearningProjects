{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFV3bzPqXFEQVeUMnjwEVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprksh-crypto/MachineLearningProjects/blob/Classification/buildAndTrainModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount drive again.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#for authorisation\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Drive files in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#Bringing over the feature descriptions and parse_example protocol from 3.2.1.\n",
        "feature_description= {\"tickers\": tf.io.FixedLenFeature([188], tf.float32, default_value=np.zeros(188)),\n",
        "                      \"weekday\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "                      \"hour\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "                      \"month\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "                      \"target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
        "\n",
        "def parse_example(serialised):\n",
        "  examples_parsed=tf.io.parse_example(serialised, feature_description)\n",
        "  target=examples_parsed.pop('target')\n",
        "  return examples_parsed, target\n",
        "\n",
        "#and the dataset that we put through parse_example.\n",
        "filepath=['/content/drive/My Drive/dataset.tfrecord']\n",
        "\n",
        "dataset=tf.data.TFRecordDataset(filepath)\n",
        "datLen=dataset.reduce(0,lambda x,y: x+1)\n",
        "\n",
        "n_valid=int(datLen.numpy()*.1)\n",
        "n_test=int(datLen.numpy()*.2)\n",
        "n_train=datLen.numpy()-n_valid-n_test\n",
        "\n",
        "dataset_train=dataset.take(n_train).batch(4096).map(parse_example,num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
        "dataset_test=dataset.skip(n_train).take(n_test).batch(4096).map(parse_example,num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
        "dataset_validation=dataset.skip(n_train+n_test).take(n_valid).batch(4096).map(parse_example,num_parallel_calls=tf.data.AUTOTUNE).cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYvcHQZaK3ar",
        "outputId": "e0b7e735-6282-4ef5-b010-3d4ee0e145eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " appml-assignment1-dataset-v2.pkl   'Essay Drafts'\n",
            "'Assignment 1 ML.ipynb'\t\t    'ML textbook code 1.ipynb'\n",
            " buildAndTrainModel\t\t     mySavedModel\n",
            " buildAndTrainModel.py\t\t    'Physics Project'\n",
            " correct.tfrecord\t\t    'Physics Scripts'\n",
            " createSavedDataset\t\t     Pics\n",
            " customImputeLayerDefinition.ipynb   testhw1\n",
            " dataset.tfrecord\t\t     Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HO8zI6j_AQs"
      },
      "outputs": [],
      "source": [
        "#3.3.1 asks to feed the categorical attribute inputs (weekday,hour,month) into Embedding layers of a dimension of your selection.\n",
        "\n",
        "#doesn't need to be done for weekday and hour. Only month, because we have 1-12 with an empty 0.\n",
        "\n",
        "nTokens=12 # inspected the number of values of months.\n",
        "\n",
        "catEncoder=tf.keras.layers.IntegerLookup(max_tokens=nTokens,num_oov_indices=0)\n",
        "catEncoder.adapt(dataset_train.map(lambda x,y:x['month']))\n",
        "\n",
        "#Print to see if working.\n",
        "#for el in manipulated_dataset.map(lambda x, y:x['month']).take(1):\n",
        "#  print(el)\n",
        "#  print(catEncoder(el))\n",
        "\n",
        "#Bring over keras input dictionary from 3.2.4.\n",
        "\n",
        "keras_inputs_Dict={'tickers': tf.keras.Input((188), dtype=tf.float32),\n",
        "                   'weekday': tf.keras.Input((),dtype=tf.int64),\n",
        "                   'hour': tf.keras.Input((),dtype=tf.int64),\n",
        "                   'month': tf.keras.Input((),dtype=tf.int64)}\n",
        "\n",
        "catInts=catEncoder(keras_inputs_Dict['month'])\n",
        "embDim=32\n",
        "catEmbs=tf.keras.layers.Embedding(nTokens,embDim)(catInts)\n",
        "catEmbs1=tf.keras.layers.Embedding(24,embDim)(keras_inputs_Dict['hour'])\n",
        "catEmbs2=tf.keras.layers.Embedding(7,embDim)(keras_inputs_Dict['weekday'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bringing over the myImputer class, and where the myImp instance was created.\n",
        "import tensorflow as tf\n",
        "\n",
        "class myImputer(tf.keras.layers.experimental.preprocessing.PreprocessingLayer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__( **kwargs)\n",
        "  def build(self,batch_input_shape):\n",
        "    self.means=self.add_weight(name='means',shape=(batch_input_shape[-1]), initializer=\"zeros\",trainable=False)\n",
        "    super().build(batch_input_shape)\n",
        "  def call(self, X):\n",
        "    return tf.where(tf.math.is_nan(X),self.means,X)\n",
        "  def adapt(self, dataset):\n",
        "    self.build(dataset.element_spec.shape)\n",
        "    sumOfNonNaNs=dataset.map(lambda z: tf.where(tf.math.is_nan(z), tf.zeros_like(z),z)).reduce(tf.zeros_like(self.means), lambda x,y: x+tf.reduce_sum(y,axis=0))\n",
        "    numberOfNonNaNs=dataset.map(lambda z: tf.where(tf.math.is_nan(z),tf.zeros_like(z),tf.ones_like(z))).reduce(tf.zeros_like(self.means),lambda x,y: x+tf.reduce_sum(y,axis=0))\n",
        "    self.means.assign(tf.math.divide(sumOfNonNaNs,numberOfNonNaNs))\n",
        "  def compute_output_shape(self,batch_input_shape):\n",
        "    return batch_input_shape\n",
        "\n",
        "myImp=myImputer()\n",
        "myImp.adapt(dataset_train.map(lambda x,y: x['tickers']))"
      ],
      "metadata": {
        "id": "Im4xIPrK6VeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.3.2 asks to stack the outputs of the embedding layers and the normaliser to form an input to subsequent layers using tf.concat.\n",
        "\n",
        "#Bringing over the Keras preprocessing normalization from 3.2.6.\n",
        "\n",
        "#normalisation layer\n",
        "normalizer=tf.keras.layers.Normalization(axis=-1)\n",
        "#targets need to be removed, because we don't want adapt to learn the weights for it.\n",
        "normalizer.adapt(dataset_train.map(lambda x,y:myImp(x['tickers'])))\n",
        "\n",
        "preproced=tf.concat([normalizer(myImp(keras_inputs_Dict['tickers'])),catEmbs, catEmbs1, catEmbs2],axis=-1)"
      ],
      "metadata": {
        "id": "djtS9mwaALJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.3.3 Include subsequent hidden layers as you wish, but the final output layer should have 23 (22) outputs corresponding\n",
        "#to the probabilities of the 23 (22) bins of the fractional change and utilize softmax activation\n",
        "\n",
        "#restMod=tf.keras.Sequential([tf.keras.layers.Dense(22, activation= 'softmax')])\n",
        "\n",
        "#add hidden and dropout layers?\n",
        "#restMod=tf.keras.Sequential([tf.keras.layers.Dense(300, activation=\"relu\"), tf.keras.layers.Dense(22, activation= 'softmax')])\n",
        "restMod=tf.keras.Sequential([tf.keras.layers.Dense(100, activation=\"sigmoid\"), tf.keras.layers.Dense(100, activation=\"tanh\"), tf.keras.layers.Dense(22, activation= 'softmax')])\n",
        "#restMod=tf.keras.Sequential([tf.keras.layers.Dense(300, activation=\"relu\"),tf.keras.layers.Dropout(rate=0.3), tf.keras.layers.Dense(22, activation= 'softmax')])"
      ],
      "metadata": {
        "id": "c7qxhCzyBM1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The keras model is then defined to stretch all the way from the inputs to the 23 outputs."
      ],
      "metadata": {
        "id": "k9jgznF1I_y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.3.5 Compile and fit the model in a manner of your choice.\n",
        "\n",
        "#myOpt=tf.keras.optimizers.SGD(learning_rate=1)\n",
        "#keras tuner\n",
        "#keras hyperparameters optimisation\n",
        "\n",
        "decs=restMod(preproced)\n",
        "whole_model=tf.keras.Model(inputs=keras_inputs_Dict,outputs=decs)\n",
        "whole_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "whole_model.fit(dataset_train, epochs=110, validation_data= dataset_validation, callbacks=[tf.keras.callbacks.EarlyStopping(patience=40,restore_best_weights=True)])\n",
        "whole_model.evaluate(dataset_test)\n"
      ],
      "metadata": {
        "id": "YCEjaasGJHp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff5436b-44e6-42d4-8502-b1b73e125d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/110\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 2.2939 - accuracy: 0.2066 - val_loss: 2.2984 - val_accuracy: 0.2081\n",
            "Epoch 2/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2938 - accuracy: 0.2066 - val_loss: 2.2983 - val_accuracy: 0.2081\n",
            "Epoch 3/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2938 - accuracy: 0.2066 - val_loss: 2.2983 - val_accuracy: 0.2084\n",
            "Epoch 4/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2937 - accuracy: 0.2067 - val_loss: 2.2982 - val_accuracy: 0.2089\n",
            "Epoch 5/110\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 2.2936 - accuracy: 0.2066 - val_loss: 2.2981 - val_accuracy: 0.2086\n",
            "Epoch 6/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2935 - accuracy: 0.2064 - val_loss: 2.2981 - val_accuracy: 0.2089\n",
            "Epoch 7/110\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 2.2935 - accuracy: 0.2064 - val_loss: 2.2980 - val_accuracy: 0.2091\n",
            "Epoch 8/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2934 - accuracy: 0.2064 - val_loss: 2.2979 - val_accuracy: 0.2094\n",
            "Epoch 9/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2933 - accuracy: 0.2064 - val_loss: 2.2979 - val_accuracy: 0.2094\n",
            "Epoch 10/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2933 - accuracy: 0.2066 - val_loss: 2.2978 - val_accuracy: 0.2096\n",
            "Epoch 11/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2932 - accuracy: 0.2068 - val_loss: 2.2978 - val_accuracy: 0.2096\n",
            "Epoch 12/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2931 - accuracy: 0.2069 - val_loss: 2.2977 - val_accuracy: 0.2096\n",
            "Epoch 13/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2931 - accuracy: 0.2069 - val_loss: 2.2976 - val_accuracy: 0.2096\n",
            "Epoch 14/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2930 - accuracy: 0.2069 - val_loss: 2.2976 - val_accuracy: 0.2096\n",
            "Epoch 15/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2929 - accuracy: 0.2070 - val_loss: 2.2975 - val_accuracy: 0.2099\n",
            "Epoch 16/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2929 - accuracy: 0.2069 - val_loss: 2.2975 - val_accuracy: 0.2096\n",
            "Epoch 17/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2928 - accuracy: 0.2071 - val_loss: 2.2974 - val_accuracy: 0.2094\n",
            "Epoch 18/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2927 - accuracy: 0.2070 - val_loss: 2.2973 - val_accuracy: 0.2091\n",
            "Epoch 19/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2927 - accuracy: 0.2070 - val_loss: 2.2973 - val_accuracy: 0.2091\n",
            "Epoch 20/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2926 - accuracy: 0.2071 - val_loss: 2.2972 - val_accuracy: 0.2091\n",
            "Epoch 21/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2926 - accuracy: 0.2070 - val_loss: 2.2972 - val_accuracy: 0.2094\n",
            "Epoch 22/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2925 - accuracy: 0.2070 - val_loss: 2.2971 - val_accuracy: 0.2094\n",
            "Epoch 23/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2924 - accuracy: 0.2070 - val_loss: 2.2971 - val_accuracy: 0.2099\n",
            "Epoch 24/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2924 - accuracy: 0.2073 - val_loss: 2.2970 - val_accuracy: 0.2099\n",
            "Epoch 25/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2923 - accuracy: 0.2073 - val_loss: 2.2970 - val_accuracy: 0.2096\n",
            "Epoch 26/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2923 - accuracy: 0.2074 - val_loss: 2.2969 - val_accuracy: 0.2099\n",
            "Epoch 27/110\n",
            "7/7 [==============================] - 0s 76ms/step - loss: 2.2922 - accuracy: 0.2073 - val_loss: 2.2969 - val_accuracy: 0.2099\n",
            "Epoch 28/110\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 2.2922 - accuracy: 0.2074 - val_loss: 2.2968 - val_accuracy: 0.2096\n",
            "Epoch 29/110\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 2.2921 - accuracy: 0.2075 - val_loss: 2.2968 - val_accuracy: 0.2096\n",
            "Epoch 30/110\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 2.2920 - accuracy: 0.2074 - val_loss: 2.2967 - val_accuracy: 0.2096\n",
            "Epoch 31/110\n",
            "7/7 [==============================] - 1s 82ms/step - loss: 2.2920 - accuracy: 0.2074 - val_loss: 2.2967 - val_accuracy: 0.2096\n",
            "Epoch 32/110\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 2.2919 - accuracy: 0.2073 - val_loss: 2.2966 - val_accuracy: 0.2096\n",
            "Epoch 33/110\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 2.2919 - accuracy: 0.2073 - val_loss: 2.2966 - val_accuracy: 0.2099\n",
            "Epoch 34/110\n",
            "7/7 [==============================] - 1s 85ms/step - loss: 2.2918 - accuracy: 0.2072 - val_loss: 2.2965 - val_accuracy: 0.2091\n",
            "Epoch 35/110\n",
            "7/7 [==============================] - 1s 84ms/step - loss: 2.2918 - accuracy: 0.2074 - val_loss: 2.2965 - val_accuracy: 0.2099\n",
            "Epoch 36/110\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 2.2917 - accuracy: 0.2074 - val_loss: 2.2964 - val_accuracy: 0.2094\n",
            "Epoch 37/110\n",
            "7/7 [==============================] - 1s 82ms/step - loss: 2.2917 - accuracy: 0.2074 - val_loss: 2.2964 - val_accuracy: 0.2091\n",
            "Epoch 38/110\n",
            "7/7 [==============================] - 1s 82ms/step - loss: 2.2916 - accuracy: 0.2074 - val_loss: 2.2963 - val_accuracy: 0.2094\n",
            "Epoch 39/110\n",
            "7/7 [==============================] - 1s 84ms/step - loss: 2.2916 - accuracy: 0.2074 - val_loss: 2.2963 - val_accuracy: 0.2094\n",
            "Epoch 40/110\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 2.2915 - accuracy: 0.2074 - val_loss: 2.2963 - val_accuracy: 0.2094\n",
            "Epoch 41/110\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 2.2915 - accuracy: 0.2074 - val_loss: 2.2962 - val_accuracy: 0.2099\n",
            "Epoch 42/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2914 - accuracy: 0.2075 - val_loss: 2.2962 - val_accuracy: 0.2096\n",
            "Epoch 43/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2914 - accuracy: 0.2075 - val_loss: 2.2961 - val_accuracy: 0.2096\n",
            "Epoch 44/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2913 - accuracy: 0.2075 - val_loss: 2.2961 - val_accuracy: 0.2099\n",
            "Epoch 45/110\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 2.2913 - accuracy: 0.2075 - val_loss: 2.2960 - val_accuracy: 0.2099\n",
            "Epoch 46/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2912 - accuracy: 0.2075 - val_loss: 2.2960 - val_accuracy: 0.2101\n",
            "Epoch 47/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2912 - accuracy: 0.2075 - val_loss: 2.2960 - val_accuracy: 0.2099\n",
            "Epoch 48/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2912 - accuracy: 0.2075 - val_loss: 2.2959 - val_accuracy: 0.2099\n",
            "Epoch 49/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2911 - accuracy: 0.2076 - val_loss: 2.2959 - val_accuracy: 0.2096\n",
            "Epoch 50/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2911 - accuracy: 0.2078 - val_loss: 2.2958 - val_accuracy: 0.2101\n",
            "Epoch 51/110\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 2.2910 - accuracy: 0.2077 - val_loss: 2.2958 - val_accuracy: 0.2101\n",
            "Epoch 52/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2910 - accuracy: 0.2077 - val_loss: 2.2958 - val_accuracy: 0.2101\n",
            "Epoch 53/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2909 - accuracy: 0.2077 - val_loss: 2.2957 - val_accuracy: 0.2106\n",
            "Epoch 54/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2909 - accuracy: 0.2077 - val_loss: 2.2957 - val_accuracy: 0.2104\n",
            "Epoch 55/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2908 - accuracy: 0.2077 - val_loss: 2.2956 - val_accuracy: 0.2104\n",
            "Epoch 56/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2908 - accuracy: 0.2077 - val_loss: 2.2956 - val_accuracy: 0.2104\n",
            "Epoch 57/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2908 - accuracy: 0.2077 - val_loss: 2.2956 - val_accuracy: 0.2101\n",
            "Epoch 58/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2907 - accuracy: 0.2078 - val_loss: 2.2955 - val_accuracy: 0.2101\n",
            "Epoch 59/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2907 - accuracy: 0.2077 - val_loss: 2.2955 - val_accuracy: 0.2099\n",
            "Epoch 60/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2906 - accuracy: 0.2078 - val_loss: 2.2954 - val_accuracy: 0.2101\n",
            "Epoch 61/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2906 - accuracy: 0.2078 - val_loss: 2.2954 - val_accuracy: 0.2099\n",
            "Epoch 62/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2905 - accuracy: 0.2079 - val_loss: 2.2954 - val_accuracy: 0.2099\n",
            "Epoch 63/110\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 2.2905 - accuracy: 0.2078 - val_loss: 2.2953 - val_accuracy: 0.2099\n",
            "Epoch 64/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2905 - accuracy: 0.2077 - val_loss: 2.2953 - val_accuracy: 0.2099\n",
            "Epoch 65/110\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 2.2904 - accuracy: 0.2076 - val_loss: 2.2953 - val_accuracy: 0.2101\n",
            "Epoch 66/110\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 2.2904 - accuracy: 0.2076 - val_loss: 2.2952 - val_accuracy: 0.2101\n",
            "Epoch 67/110\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 2.2903 - accuracy: 0.2075 - val_loss: 2.2952 - val_accuracy: 0.2099\n",
            "Epoch 68/110\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 2.2903 - accuracy: 0.2076 - val_loss: 2.2952 - val_accuracy: 0.2099\n",
            "Epoch 69/110\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 2.2903 - accuracy: 0.2075 - val_loss: 2.2951 - val_accuracy: 0.2099\n",
            "Epoch 70/110\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 2.2902 - accuracy: 0.2076 - val_loss: 2.2951 - val_accuracy: 0.2099\n",
            "Epoch 71/110\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 2.2902 - accuracy: 0.2077 - val_loss: 2.2951 - val_accuracy: 0.2099\n",
            "Epoch 72/110\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 2.2902 - accuracy: 0.2078 - val_loss: 2.2950 - val_accuracy: 0.2096\n",
            "Epoch 73/110\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 2.2901 - accuracy: 0.2078 - val_loss: 2.2950 - val_accuracy: 0.2096\n",
            "Epoch 74/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2901 - accuracy: 0.2078 - val_loss: 2.2950 - val_accuracy: 0.2101\n",
            "Epoch 75/110\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 2.2900 - accuracy: 0.2079 - val_loss: 2.2949 - val_accuracy: 0.2099\n",
            "Epoch 76/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2900 - accuracy: 0.2080 - val_loss: 2.2949 - val_accuracy: 0.2104\n",
            "Epoch 77/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2900 - accuracy: 0.2081 - val_loss: 2.2949 - val_accuracy: 0.2104\n",
            "Epoch 78/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2899 - accuracy: 0.2082 - val_loss: 2.2948 - val_accuracy: 0.2101\n",
            "Epoch 79/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2899 - accuracy: 0.2082 - val_loss: 2.2948 - val_accuracy: 0.2101\n",
            "Epoch 80/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2899 - accuracy: 0.2081 - val_loss: 2.2948 - val_accuracy: 0.2099\n",
            "Epoch 81/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2898 - accuracy: 0.2081 - val_loss: 2.2947 - val_accuracy: 0.2101\n",
            "Epoch 82/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2898 - accuracy: 0.2082 - val_loss: 2.2947 - val_accuracy: 0.2104\n",
            "Epoch 83/110\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 2.2898 - accuracy: 0.2084 - val_loss: 2.2947 - val_accuracy: 0.2104\n",
            "Epoch 84/110\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 2.2897 - accuracy: 0.2085 - val_loss: 2.2946 - val_accuracy: 0.2104\n",
            "Epoch 85/110\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 2.2897 - accuracy: 0.2085 - val_loss: 2.2946 - val_accuracy: 0.2104\n",
            "Epoch 86/110\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 2.2897 - accuracy: 0.2085 - val_loss: 2.2946 - val_accuracy: 0.2101\n",
            "Epoch 87/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2896 - accuracy: 0.2086 - val_loss: 2.2946 - val_accuracy: 0.2104\n",
            "Epoch 88/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2896 - accuracy: 0.2084 - val_loss: 2.2945 - val_accuracy: 0.2104\n",
            "Epoch 89/110\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 2.2896 - accuracy: 0.2085 - val_loss: 2.2945 - val_accuracy: 0.2104\n",
            "Epoch 90/110\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 2.2895 - accuracy: 0.2085 - val_loss: 2.2945 - val_accuracy: 0.2104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extras: Figuring out the best batch size.\n",
        "#import time\n",
        "\n",
        "#secsReqd=[]\n",
        "\n",
        "#for batchSize in [32,64,128,512,1024,2048,4096,8*1024]:\n",
        "#  dataset=tf.data.TFRecordDataset(filepath).batch(batchSize).map(parse_example,num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
        "#  start_time=time.time()\n",
        "#  whole_model.fit(dataset,epochs=5,verbose=0)\n",
        "#  secsReqd.append((time.time()-start_time))\n",
        "#  print(secsReqd)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EnFn2Cw9i9Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#whole_model.save('/content/drive/My Drive/mySavedModel')\n",
        "\n",
        "#tf.saved_model.save(whole_model, '/content/drive/My Drive/mySavedModel')"
      ],
      "metadata": {
        "id": "aSeLFL7fnKKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#My notes:\n",
        "\n",
        "#Starting with batching at 64, without any hidden layers and without a dropout layer, I got an accuracy of 25%.\n",
        "#   First mess with batching:\n",
        "#       @ 32 >>>> too slow, doesn't hit 25% either, around 24%\n",
        "#       @ 64 >>>> sweet spot.\n",
        "#       @ 128 >>> worse than 32, 23%\n",
        "\n",
        "#           If I run this bare bones code, and try to run the \"batch optimiser\" code, it recommends the larger numbers: 4096, 8*1024, 1024, and 512. In that order.\n",
        "\n",
        "#       @ 4096 >> this is abismal, accuracy of 15%\n",
        "#                   >> if then I tweak this, increase # of epochs from 30 to 100, this is consistently at 25%\n"
      ],
      "metadata": {
        "id": "AihDTjk88VZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}